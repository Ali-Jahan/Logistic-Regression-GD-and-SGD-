{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_GD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuCwBikN844d",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qAXqnKI84ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import mnist\n",
        "\n",
        "\n",
        "class RidgeReg:\n",
        "    maxIter = 200\n",
        "\n",
        "    def load_dataset(self):\n",
        "        mndata = mnist.MNIST(\"data/\")\n",
        "        X_train_, labels_train = map(np.array, mndata.load_training())\n",
        "        X_test_, labels_test = map(np.array, mndata.load_testing())\n",
        "        Y_train_ = labels_train\n",
        "        Y_test_ = labels_test\n",
        "        X_train_ = X_train_ / 255.0\n",
        "        X_test_ = X_test_ / 255.0\n",
        "\n",
        "        return X_train_, Y_train_, X_test_, Y_test_\n",
        "\n",
        "    def jValue(self, X, Y, w, b, lambda_):\n",
        "        return np.mean(np.log(1 + np.exp(-Y * (b + np.dot(w, X.T))))) \\\n",
        "               + lambda_ * np.sum(np.square(w))\n",
        "\n",
        "    def mu(self, X, Y, w, b):\n",
        "        return 1 / (1 + np.exp(-Y * (b + np.matmul(X, w))))\n",
        "\n",
        "    def J_w(self, X, Y, w, b, lambda_, size):\n",
        "        return (np.dot((-Y * (1 - self.mu(X, Y, w, b))), X)) / size \\\n",
        "               + 2 * lambda_ * w\n",
        "\n",
        "    def J_b(self, X, Y, w, b):\n",
        "        return np.mean(-Y * (1 - self.mu(X, Y, w, b)))\n",
        "\n",
        "    # get the proportion of wrong answers\n",
        "    def error(self, y1, y2):\n",
        "        r = np.equal(y1, y2)\n",
        "        return 1 - (np.count_nonzero(r) / len(y1))\n",
        "\n",
        "    # gradient descent\n",
        "    def gd(self, X, X_test_, Y, Y_test_, step_, lambda_):\n",
        "        b = 0\n",
        "        jtrain_list = np.empty(self.maxIter)\n",
        "        jtest_list = np.empty(self.maxIter)\n",
        "        error_train_list = np.empty(self.maxIter)\n",
        "        error_test_list = np.empty(self.maxIter)\n",
        "        iter_list = np.empty(self.maxIter)\n",
        "        n = len(Y)\n",
        "        w = np.zeros(np.shape(X)[1])\n",
        "        for i in range(1, self.maxIter + 1):\n",
        "            wgrad = step_ * self.J_w(X, Y, w, b, lambda_, n)\n",
        "            bgrad = step_ * self.J_b(X, Y, w, b)\n",
        "            w = w - wgrad\n",
        "            b = b - bgrad\n",
        "            jtrain_list[i - 1] = self.jValue(X, Y, w, b, lambda_)\n",
        "            jtest_list[i - 1] = self.jValue(X_test_, Y_test_, w, b, lambda_)\n",
        "            y_new_train = self.toSign(X, w, b)\n",
        "            y_new_test = self.toSign(X_test_, w, b)\n",
        "            error_train_list[i - 1] = self.error(y_new_train, Y)\n",
        "            error_test_list[i - 1] = self.error(y_new_test, Y_test_)\n",
        "            iter_list[i - 1] = i\n",
        "\n",
        "        return jtrain_list, jtest_list, iter_list, error_train_list, error_test_list\n",
        "\n",
        "    def toSign(self, X, w, b):\n",
        "        return np.sign(np.matmul(X, w) + b)\n",
        "        \n",
        "R = RidgeReg()\n",
        "X_train, Y_train, X_test, Y_test = R.load_dataset()\n",
        "\n",
        "X_train = np.delete(X_train, np.where((Y_train != 2) & (Y_train != 7)), axis=0)\n",
        "Y_train = np.delete(Y_train, np.where((Y_train != 2) & (Y_train != 7)))\n",
        "# the default data type was unsigned int, so changed it to regular int\n",
        "Y_train = Y_train.astype(np.int8)\n",
        "Y_train[Y_train == 7] = 1\n",
        "Y_train[Y_train == 2] = -1\n",
        "# Now the test set\n",
        "X_test = np.delete(X_test, np.where((Y_test != 2) & (Y_test != 7)), axis=0)\n",
        "Y_test = np.delete(Y_test, np.where((Y_test != 2) & (Y_test != 7)))\n",
        "Y_test = Y_test.astype(np.int8)\n",
        "Y_test[Y_test == 7] = 1\n",
        "Y_test[Y_test == 2] = -1\n",
        "\n",
        "_lambda = 0.1\n",
        "step = 0.05\n",
        "\n",
        "# plotting J(w,b) as function of iteration for test and training sets\n",
        "\n",
        "# Gradient Descent\n",
        "jtrain, jtest, iters, errTrain, errTest = R.gd(X_train, X_test, Y_train, Y_test, step, _lambda)\n",
        "plt.plot(iters, jtrain)\n",
        "plt.plot(iters, jtest)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('J(w,b)')\n",
        "plt.legend(['Training set', 'Test set'])\n",
        "plt.title('(GD) Value of J(w,b) by iteration (A6.b.I)')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(iters, errTrain)\n",
        "plt.plot(iters, errTest)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('Proportion of wrong predictions')\n",
        "plt.legend(['Training set Error', 'Test set Error'])\n",
        "plt.title('(GD) Misclassification Error (A6.b.II)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}